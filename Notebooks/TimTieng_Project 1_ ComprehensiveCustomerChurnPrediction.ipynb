{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LP2OENPlk1mm"
   },
   "source": [
    "# Project 1: Comprehensive Customer Churn Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITQiKNUJk0zP"
   },
   "source": [
    "## Project Overview\n",
    "\n",
    "In this project activity, you will dive into analyzing customer data from a business to predict churn risk comprehensively. You will not only apply traditional supervised learning algorithms such as decision trees and random forests but also explore logistic regression, support vector machines (SVMs), and k-nearest neighbors (KNN) to identify key churn factors. This expanded scope will enhance your understanding of different model assumptions, strengths, and weaknesses, enabling you to build a robust model to target potential churners effectively.\n",
    "\n",
    "In this project, you will:\n",
    "- **Evaluate** different supervised learning algorithms to understand their suitability for churn prediction.\n",
    "- **Implement** data preprocessing techniques tailored to the requirements of each algorithm.\n",
    "- **Optimize** model parameters to improve prediction accuracy.\n",
    "- **Interpret** the model outcomes to extract actionable insights for retention campaigns.\n",
    "\n",
    "**Estimated Completion Time**\n",
    "\n",
    "12 to 14 hours\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Information\n",
    "    Python Version: 3.10.13\n",
    "    Python Packages: Outlined in pip_requirements.txt\n",
    "    Dataset: WA_Fn-UseC_-Telco-Customer-Churn.csv (Provided)\n",
    "    File Authors: Tim Tieng, Scott Mayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HYXxqMjlkvc-"
   },
   "outputs": [],
   "source": [
    "# Import Packages: \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Algorithms, Modeling and preprocessing packages\n",
    "import feature_engine\n",
    "from scipy.stats import anderson\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# Deep Learning\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qo2QVaaYlLdQ"
   },
   "source": [
    "## Task 1: Initial Data Exploration\n",
    "\n",
    "**Objective:** Load the dataset and perform an initial examination to understand its structure and identify any immediate cleaning needs.\n",
    "\n",
    "**Activities:**\n",
    "\n",
    "1. Load the dataset using pandas and display the first few rows to get an initial understanding of the data.\n",
    "2. Examine the dataset's shape to understand the scale of the data we're dealing with.\n",
    "3. Check the data types of each column to identify which are categorical and which are numerical.\n",
    "4. Identify any missing values in the dataset.\n",
    "5. Generate summary statistics for numerical columns to identify any immediate anomalies or outliers.\n",
    "\n",
    "\n",
    "**Estimated Completion Time:** 60 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p2xWQGzLDf58"
   },
   "source": [
    "**Hints:**\n",
    "\n",
    "* Use `pd.read_csv()` to load the dataset. Remember to `import pandas as pd`.\n",
    "* Use `.head()`, `.info()`, `.dtypes`, `.isnull().mean()`, and .`describe()` methods to explore the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1: Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain - Read in the data, convert to PD Dataframe, and perform initial inspection of the dataset\n",
    "churn_file = pd.read_csv('../Data/WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
    "churn_df = pd.DataFrame(churn_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2: Examine the Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(churn_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "1. Rows/Observations: 7043 \n",
    "2. Attributes/Columns: 21\n",
    "3. Datatypes Present: int64(2x), float64(1x), string objects(18x)\n",
    "4. Memory Usage - 7.8 MB (Small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.3: Check Datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial inspect - Provde information on Attribute Names, Non-Null Count, Data Types, Memory Usage\n",
    "churn_df.info(memory_usage='deep')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.4: Check for Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations \n",
    "TotalCharges has some values with whitespace strings. These need replaced with 0s. We will also change the SeniorCitizen Column to a string because although it contains 0s and 1s, it is a categorical column. Lastly, we are changing TotalCharges to a numerical column because it contains numerical data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null Values - percentage per column\n",
    "churn_df.isna().mean().sort_values(ascending= False)\n",
    "# No null values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df.TotalCharges = churn_df['TotalCharges'].replace(' ', 0)\n",
    "churn_df['SeniorCitizen'] = churn_df['SeniorCitizen'].astype('str')\n",
    "churn_df['TotalCharges'] = pd.to_numeric(churn_df['TotalCharges'])\n",
    "churn_df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.5: Generate Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intial Inspection of data using head()\n",
    "churn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desriptive Statistics -Numerical Attributes\n",
    "churn_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive Statistics - Categorical Values included\n",
    "churn_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Observerations\n",
    "1. Gender Distribution - Dataset has 7043 total observations with only two unique values (Male, Female). Right now there are 3555 Male values Which suggest the data is relatively balanced. \n",
    "2. Senior Citizen Attribute (Prior to DT casting) - The mean value is .162 which can be translated as 16.2 of the userbase is classified as a senior citizen. Additional definition of what describes a Senior Citizen mayh be needed for future analysis (what age classifies senior citizen cut off?)\n",
    "3. Tenure - Mean value is about 32.4% with a min value of 0 months and max months membership of 72 months. STD is 24.6 which can be viewed as a wide variance in the quartiles.\n",
    "4. XService Attributes - This showcases that this one company has multiple offerings  that can appeal to different customer bases. From a business perspective, we may need to study customer propensity to include one service based on the services the customer currently pays for. This can increase market capture for the business writ-large. (Bundling services to a subscription)\n",
    "5. InternetService - Fiber Optic is the most frequent value in this column representing that almost half of the customer base pays for fiber optic services. This could be a business opportunity to focus on members with lower levels of internet service to upgrade potentially reducing the likliness of leaving our service for a competitor.\n",
    "5. Churn - Top value is \"no\" with a frequency count of 5174/7043. This attribute should be used in future analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jzs6jHL_bAHA"
   },
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Y-dErPdtjIA"
   },
   "source": [
    "## Task 2: Exploratory Data Analysis (EDA)\n",
    "\n",
    "**Objective:** Use statistical analysis and visualization techniques to uncover insights and identify patterns related to custom\ter churn.\n",
    "\n",
    "**Activities:**\n",
    "\n",
    "1. **Visualize distribution of numerical features** to identify any skewed data or outliers.\n",
    "2. **Analyze churn rate by categorical features** to uncover any patterns that may indicate a higher likelihood of churn.\n",
    "3. **Examine relationships between features** using correlation matrices and scatter plots for numerical features, and stacked bar charts for categorical features against churn.\n",
    "4. **Use box plots to identify outliers** in numerical data and understand distributions across churned and retained customers.\n",
    "5. **Create a pair plot** to visualize the pairwise relationships and distributions of numerical features segmented by churn.\n",
    "\n",
    "\n",
    "**Estimated Completion Time:** 90 minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6DqZsr66EyvU"
   },
   "source": [
    "**Hints:**\n",
    "\n",
    "* Use `sns.histplot()` for distributions, `sns.countplot()` for categorical data analysis, `sns.heatmap()` for correlation matrices, `sns.boxplot()` for outliers, and `sns.pairplot()` for pairwise relationships.\n",
    "* Remember to import necessary libraries like `seaborn as sns` and `matplotlib.pyplot as plt`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1 Visualize distribution of numerical features\n",
    "Created a function to visualize numerical features. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_lX5pshylwlE"
   },
   "outputs": [],
   "source": [
    "def visualize_numerical_histograms(df):\n",
    "    \"\"\"\n",
    "    Purpose - A function that takes in a dataframe and returns histograms on the dataframe's numerical values. \n",
    "              This helps with Exploratory Data Analysis (EDA).\n",
    "    Parameters - Pandas Dataframe\n",
    "    Returns - Nothing\n",
    "    Prints - Histograms to highlight distributions\n",
    "    \"\"\"\n",
    "    # Create numerical only dataframe based on datatypes in the info() output\n",
    "    numerical_only_df = churn_df.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "    # Iterate\n",
    "    for column in numerical_only_df.columns:\n",
    "        # format the visualization\n",
    "        plt.figure(figsize=(10,6))\n",
    "        sns.histplot(data=numerical_only_df, x=column, kde=True, bins= 20)\n",
    "        plt.title(f\"Distribution of {column} Column of Churn Dataset\")\n",
    "        plt.xlabel = column\n",
    "        plt.ylabel = 'Frequency'\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test and Verifify\n",
    "visualize_numerical_histograms(churn_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visual Observations\n",
    "1. **Tenure Distribution**: There are two distinct peaks for this attribute, alluding to that the current customer base are either new OR customers that churn quickly. The center dip in the graph may allude to the typical range where churn is likely to happen\n",
    "2. **MonthlyCharges Distribution**: Visually, there is a right skew to this column. There also seems to be a lot of customers who are only paying for $20 for services. This may highlight that these customers are only paying for a single service (internet, phone, etc). From a business/market capture perspective, we can view this as potential customer base to target with the goal of motivating them to add more services to their subscription. The right end of the graph may represent customers who pay for premium-like services or combines multiple service offerings to their subscription. \n",
    "3. **TotalCharges Distributio**n**: This attribute has a long trailing tail towards the right of the graph. Additionally, there are alot of customers with a low \"TotalCharges\" value. This is aligned with teh MonthlyCharges column and may be attributed to the customer base only paying for 1 service. \n",
    "\n",
    "**Business Significance**: The three graphs shows information abou the current customer base. It seems as though there are two \"populations\" in the customer base: Customers who are new, and customers who pay for multiple services. Based on the dip in the MonthlyCharges Distribution, this could be viewed as our customberbase we should focus on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistical Observations: Anderson Tests for Skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anderson_skewness_test(df):\n",
    "  \"\"\"\n",
    "  Purpose - Conducts anderson skewness test on numerical columns to determine if column is normally distrubuted\n",
    "  Parameters - Pandas Dataframe\n",
    "  Returns - Nothing\n",
    "  Prints - column name, anderson value, whether or not column is normal \n",
    "  \"\"\"\n",
    "  numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "  for col in df.select_dtypes(include='number'):\n",
    "    a = anderson(df[col], dist='norm')\n",
    "    print(col + \"\\t\", end=\"\")\n",
    "    if len(col) < 8:\n",
    "      print(\"\\t\", end=\"\")\n",
    "    print(f'{a.statistic:.2f}', end=\"\\t\")\n",
    "    if a.statistic < a.critical_values[0]:\n",
    "      print(\"Normal\")\n",
    "    else:\n",
    "      print(\"Not Normal\")\n",
    "  return \n",
    "\n",
    "anderson_skewness_test(churn_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations: \n",
    "- None of our numerical columns are normal at the 15% significance level based on having anderson values above the critical value at the 15% threshold. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2 Analyze Churn by Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_churn_categorical_features(df):\n",
    "  \"\"\"\n",
    "  Purpose - To identify distribution of categories in categorical features\n",
    "  Parameters - Pandas Dataframe\n",
    "  Returns - Nothing\n",
    "  Prints - Boxplots for the provided pandas Dataframe that can be used for Exploratory Data Analysis (EDA)\n",
    "  \"\"\"\n",
    "  categorical_cols = []\n",
    "  numerical_cols = []\n",
    "  for c in df.columns:\n",
    "    if df[c].map(type).eq(str).any() or df[c].map(type).eq(str).any():\n",
    "      categorical_cols.append(c)\n",
    "    else:\n",
    "      numerical_cols.append(c)\n",
    "\n",
    "  for col in categorical_cols:\n",
    "    count = df[col].nunique() \n",
    "    print(f\"{col}\", end=\"\")\n",
    "    if len(col) < 8:\n",
    "      print(\"\\t\", end=\"\")\n",
    "    if len(col) < 16:\n",
    "      print(\"\\t\", end=\"\")\n",
    "    print(f\" Num Categories: {count}\")\n",
    "    for cat in df[col].unique():\n",
    "      if col == \"customerID\":\n",
    "        continue\n",
    "      count_cat = df[col].value_counts()[cat]  \n",
    "      print(f\"\\t{cat}: {count_cat}, {count_cat/df.shape[0]*100:.2f}%\")\n",
    "    print()\n",
    "    \n",
    "  data_numeric = df[numerical_cols]\n",
    "  data_categorical = pd.DataFrame(df[categorical_cols])\n",
    "    \n",
    "  data_joined = pd.concat([data_numeric, data_categorical], axis=1)\n",
    "    \n",
    "  data_joined.describe(include='all')\n",
    "  data_joined.head()\n",
    "  return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_churn_categorical_features(churn_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations \n",
    "- **Encoding Techniques**: Based on the distributions, it is likely a LabelEncoder is the best way to encode the various categorial columns in the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3: Use Boxplots to identify outliers in numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Box Plots\n",
    "def create_boxplots(df):\n",
    "    \"\"\"\n",
    "    Purpose - To create boxplots of a given dataframe\n",
    "    Parameters - Pandas Dataframe\n",
    "    Returns - Nothing\n",
    "    Prints - Boxplots for the provided pandas Dataframe that can be used for Exploratory Data Analysis (EDA)\n",
    "    \"\"\"\n",
    "    # Create numerical only dataframe\n",
    "    numerical_only_df = churn_df.select_dtypes(include=['float64', 'int64'])\n",
    "    # Base figure width that can be  scale based on the range of the data\n",
    "    base_width = 5\n",
    "    max_width = 15 # added to resolve ValueError: Image size of 217119x400 pixels is too large. It must be less than 2^16 in each direction.\n",
    "\n",
    "    # Iterate over each column\n",
    "    for column in numerical_only_df:\n",
    "        # Calculate the range of the column data\n",
    "        data_range = numerical_only_df[column].max() - numerical_only_df[column].min()\n",
    "        \n",
    "        # Scale the figure width, we use max to ensure a minimum width is maintained\n",
    "        figure_width = min(max(base_width, base_width * (data_range / 20)), max_width)  # Updated as part of max_width \n",
    "        \n",
    "        # Create a figure with the adjusted size\n",
    "        plt.figure(figsize=(figure_width, 4))  # Height is kept constant\n",
    "        plt.title(f'Boxplot for {column} Attribute')\n",
    "        sns.boxplot(x=df[column], orient='h')\n",
    "        \n",
    "        plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_boxplots(churn_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boxplot Observations\n",
    "1. **SeniorCitizen (Prior to casting)** - There seems to be an outlier that returns an ineffective boxplot. I need to create a function that can handle outliers. I originally thought handling the figure width was needed to resolve the error. \n",
    "2. **Tenure Boxplot** : This attrribute looks relatively uniform. This graph visually describes the customers who been wiht the services for a long time\n",
    "3. **MonthlyCharges Boxplot**: The median placement on the graph confirms the right skew nature from the histogram. This plot also confirms the observation that customers tend to have lower monthly charges or are paying for single services vs bundling services\n",
    "4. **TotalCharges Boxplot**: The skewness of this attribute is more apparent in the boxplot. The median value represents that half of the customer base is clustered towards the lower end of charges. Potential outliers as indicated by the long tail on the right side of the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix and heatmap visualization\n",
    "correlation_matrix = churn_df.corr(numeric_only=True)\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap visualization of the correlation matrix \n",
    "# Purpose of visualizing Correlation Matrix - To see the relationships more apparently \n",
    "sns.heatmap(correlation_matrix, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix and Heatmap Observations:\n",
    "1. There are positive relationships amongst the 3 numerical attributes.\n",
    "2. **SeniorCitizen vs Tenure (Prior to Casting)e** = The value is so close to 0 that it alludes to there is no relationship at all between these two attributes \n",
    "3. Tenure vs Monthly Charges - There is a slightly positive relationship between these two attributes. This suggests that as tenure increases, monthyl charges also increase slightly since they are positively correlated, although weak. This is an interesting anamoly as businesses tend to \"reward\" long-term customers or value their business throughout the years. This could be the business' approach to handle inflation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_squared_test(df, target):\n",
    "    \"\"\"\n",
    "    Purpose - to assist in EDA on categorical data. This function performs a Chi-squared test on all all categorical \n",
    "            attributes in a df against the target variable.\n",
    "    Target Variable - 'Churn'\n",
    "    Parameters- pandas dataframe with numerical and categorical data\n",
    "    Return - results of the chi2 test in the 'results' variable\n",
    "    Prints - The Chi-squared statistic and p-value for each pair of categorical columns.\n",
    "    \"\"\"\n",
    "    # Select Object dt based off the output from info()\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    significant_results = []\n",
    "\n",
    "    # Check if 'Churn' Column is present. If not, prompt the user\n",
    "    if target not in categorical_cols:\n",
    "        print(f\"The target variable {target} is not found. Please verify\")\n",
    "        return significant_results\n",
    "\n",
    "    # Perform a Chi-squared test for each pair of categorical variables\n",
    "    for col in categorical_cols:\n",
    "        if col == target:\n",
    "            continue\n",
    "        # Create a contingency table\n",
    "        contingency_table = pd.crosstab(df[col], df[target])\n",
    "\n",
    "        # Perform the Chi-squared test - chi2 value, p value, degrees of freedom and expected frequencies if null hypothesis were true\n",
    "        chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "        # Save the result IF p value is less than or equal to 0.05 - Statistically significant\n",
    "        if p <= 0.05:\n",
    "            significant_results.append({\n",
    "                'Column 1': col,\n",
    "                'Chi-squared Statistic': chi2,\n",
    "                'p-value': p\n",
    "            })\n",
    "\n",
    "            # Output the result\n",
    "            print(f\"Chi-squared test for {col} and {target}\")\n",
    "            print(f\"Chi-squared Statistic: {round(chi2,2)}, p-value: {round(p,4)}\\n\")\n",
    "\n",
    "    return significant_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test by calling the function\n",
    "chi_squared_test(churn_df, 'Churn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi-Squared On Categorical Values Observations\n",
    "\n",
    "- To identify the relationship between categorical values and our target variable of \"Churn\", we created a function to run the chi-squared test on our dataframe.\n",
    "- Low p-values across the attributes, confirming strong rejection of the null hypothesis\n",
    "- These attributes may have strong predicitve power in our future modeling\n",
    "\n",
    "**Attributes with strong Association with 'Churn' (Chi Statistic values are 500 or greater)**\n",
    "1.  InternetService\n",
    "2.  OnlineSecurity\n",
    "3.  OnlineBackup\n",
    "4.  DeviceProtection\n",
    "5.  TechSupport\n",
    "6.  PaymentMethod\n",
    "7.  Contract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.5: Create Pair Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pair_plot(df): \n",
    "    \"\"\"\n",
    "    Purpose - \n",
    "    Parameters - \n",
    "    Return - \n",
    "    Prints - \n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_pair_plot(churn_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uoz04XzUa9DI"
   },
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQ70QDBdtiRu"
   },
   "source": [
    "## Task 3: Data Preprocessing\n",
    "\n",
    "**Objective:** Split the dataset into training and testing sets, then clean the training dataset by handling missing values, outliers, and duplicate entries using `feature_engine` to prepare for further analysis.\n",
    "\n",
    "**Activities:**\n",
    "\n",
    "1. **Split the dataset into training and testing** sets to ensure a fair evaluation of the model built on processed data.\n",
    "2. **Handle missing values** in the training set using appropriate imputation techniques.\n",
    "3. **Identify and treat outliers** in numerical features of the training set to minimize their impact.\n",
    "4. **Check for and remove constant and duplicate entries** in the training set to maintain data integrity.\n",
    "5. **Apply the same preprocessing steps** (imputation, outlier handling) to the test set using parameters derived from the training set to maintain consistency and prevent data leakage.\n",
    "\n",
    "**Estimated Completion Time:** 90 minutes\n",
    "\n",
    "***Note:*** *The splitting of the dataset into training and testing sets before preprocessing is a best practice in machine learning. This approach ensures that the model is evaluated on unseen data that has been processed in the same way as the training data, without using information from the test set during the training phase.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "llKBUa5sbF1Z"
   },
   "source": [
    "**Hints:**\n",
    "\n",
    "* Use `train_test_split` from `sklearn.model_selection` to split your data.\n",
    "* Handle numerical missing values with pandas `.fillna()`, sklearn's `SimpleImputer` or feature-engine's `MeanMedianImputer`.\n",
    "* Detect and handle outliers by calculating IQR or using feature_engine's Winsorizer.\n",
    "* Remove duplicates using DataFrame's `.drop_duplicates()` method or feature-engine's `DropDuplicateFeatures()` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ifw4KHXHthI_"
   },
   "outputs": [],
   "source": [
    "# Create a copy of the original data fram for break glass scenarios\n",
    "churn_original = churn_df.copy()\n",
    "\n",
    "# Isolate target variable to its own variable with all column values\n",
    "target = churn_df['Churn']\n",
    "\n",
    "# Drop Columns\n",
    "# churn_dropped = churn_df.drop(columns=['Churn', 'customerID', 'gender', 'PaperlessBilling' ,'PaymentMethod'], axis=1, inplace= True)\n",
    "\n",
    "# Impute if Necessary\n",
    "\n",
    "# Normalize the Data\n",
    "\n",
    "# Fit and Transform\n",
    "\n",
    "# Split the Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZ2zYoA8bFvk"
   },
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uet5FuWiXIfv"
   },
   "source": [
    "## Task 4: Feature Selection and Engineering\n",
    "\n",
    "**Objective:** Create new features that might improve model performance, transform applicable features, and select the most relevant features for modeling.\n",
    "\n",
    "**Activities:**\n",
    "\n",
    "  1. **Feature Creation:** Create a feature that captures the customer's total spend relative to their tenure. This could highlight customers who might be paying more over a shorter period, potentially indicating dissatisfaction..\n",
    "  2. **Feature Transformation:**  Normalize skewed features such as MonthlyCharges and TotalCharges using a variance stabilizer to make their distribution more symmetric, which can help certain algorithms perform better.\n",
    "  3. **Feature Selection:** Use mutual information or another model-based feature selection method to identify features that have the most significant relationship with the target variable, `Churn`.\n",
    "\n",
    "**Estimated Completion Time:** 90 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xFX8Qs52X7J9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YmxMR_khcbsa"
   },
   "source": [
    "**Hints:**\n",
    "\n",
    "* Create new features based on existing data that might indicate behavioral patterns.\n",
    "* Use feature_engine's `YeoJohnsonTransformer` or manual transformations for skewed features.\n",
    "* Select relevant features based on mutual information using `SelectKBest` from sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GdJsFCpCcm72"
   },
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x7tuCkb1XIc2"
   },
   "source": [
    "## Task 5: Logistic Regression and Assumptions Validation\n",
    "\n",
    "**Objective:** Implement a logistic regression model and validate its assumptions, adjusting features as necessary.\n",
    "\n",
    "**Activities:**\n",
    "\n",
    "  1. **Fit a logistic regression model** using the selected features from Task 4 to predict customer churn.\n",
    "  2. **Validate that the relationship** between log odds and each independent variable is linear.\n",
    "  3. **Check for multicollinearity** among predictors.\n",
    "  4. **Ensure that the residuals (errors) are independent** of each other.\n",
    "  5. **Check that the variance of error terms is consistent** across all levels of the independent variables.\n",
    "  6. **Identify and address extreme outliers** that could unduly influence the model.\n",
    "  7. **Adjust features** based on the findings from assumption validations. This may involve transforming variables, removing or adding features, or addressing outliers and multicollinearity.\n",
    "  8. **Fit a new logistic regression model** and compare its results with the initial model\n",
    "\n",
    "**Estimated Completion Time:** 90 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0pchACGMYaPI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zvzwImICcv0M"
   },
   "source": [
    "**Hints:**\n",
    "\n",
    "* Fit the logistic regression model using `LogisticRegression` from sklearn.\n",
    "* Validate linearity using scatter plots or seabornâ€™s `lmplot`.\n",
    "* Check multicollinearity with VIF from the statsmodels library.\n",
    "* Assess model residuals with `residplot` from seaborn or manually plot residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0geY8gEzc8vA"
   },
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ufY2jjLXIaU"
   },
   "source": [
    "## Task 6: Decision Trees, Random Forests, and Model Complexity\n",
    "\n",
    "**Objective:** Build decision tree and random forest models, focusing on understanding and tuning model complexity to avoid overfitting.\n",
    "\n",
    "**Activities:**\n",
    "\n",
    "  1. **Use the decision tree classifier** to create a model for predicting customer churn. Focus on understanding the default model complexity and its impact on performance.\n",
    "  2. **Experiment with parameters** that control the complexity of the decision tree, such as max_depth, min_samples_split, and min_samples_leaf, to find a balance that reduces overfitting while maintaining good predictive performance.\n",
    "  3. **Implement a random forest classifier** to improve prediction accuracy and robustness by aggregating multiple decision trees.\n",
    "  4. **Adjust parameters** such as n_estimators, max_depth, and max_features to optimize the random forest model. Aim to enhance model accuracy without significant overfitting.\n",
    "  5. **Use metrics** such as accuracy, precision, recall, F1 score, and the ROC-AUC score to evaluate and compare the decision tree and random forest models.\n",
    "  6. **Investigate the features that are most influential** in predicting customer churn according to the random forest model.\n",
    "\n",
    "**Estimated Completion Time:** 90 minutes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r3aR46AeYvnq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBkc8LMydCpT"
   },
   "source": [
    "**Hints:**\n",
    "\n",
    "* Train models using `DecisionTreeClassifier` and `RandomForestClassifier` from sklearn.\n",
    "* Use `GridSearchCV` or `RandomSearchCV` for hyperparameter tuning.\n",
    "* Evaluate model performance with `sklearn.metrics` and visualize feature importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhDgD4uldCgB"
   },
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RwkZyCcdXIXs"
   },
   "source": [
    "## Task 7: SVM and KNN Implementation\n",
    "\n",
    "**Objective:** Apply SVM and KNN algorithms to the churn prediction problem, highlighting the importance of data scaling and parameter tuning.\n",
    "\n",
    "**Activities:**\n",
    "\n",
    "  1. **Scale the feature set** to ensure that SVM and KNN algorithms perform optimally, as both are sensitive to the scale of input data.\n",
    "  2. **Train an SVM model** on the scaled feature set. Start with the default hyperparameters to establish a baseline performance.\n",
    "  3. **Optimize SVM parameters** including C (regularization parameter) and kernel to improve model performance.\n",
    "  4. **Apply the KNN algorithm**, initially using a small k (e.g., 5) to model the churn prediction problem.\n",
    "  5. **Find the optimal k value** for KNN. Consider the balance between underfitting and overfitting as k changes.\n",
    "  6. **Compare the performance of SVM and KNN models** based on accuracy, precision, recall, F1 score, and ROC-AUC score. Discuss the strengths and weaknesses of each model in the context of the churn prediction problem.\n",
    "  7. For SVM models, especially linear kernel SVM, **examine the coefficients of features** to understand their impact on the prediction. Use this insight to refine the feature set and improve model simplicity and performance.\n",
    "\n",
    "**Estimated Completion Time:** 120 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uArCKf6GZM1O"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2VWJzOHAdWi1"
   },
   "source": [
    "**Hints:**\n",
    "\n",
    "* Scale features using `StandardScaler` or `RobustScaler` before applying `SVM` or `KNN`.\n",
    "* Train `SVM` using `SVC` and tune with `GridSearchCV` or with `RamdomSearchCV`.\n",
    "* Implement KNN using `KNeighborsClassifier` and find the best k through cross-validation.\n",
    "* Compare model metrics using sklearn's evaluation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wAgP24pudWZJ"
   },
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MPhbeIYkXIUe"
   },
   "source": [
    "##Task 8: Model Evaluation and Comparison\n",
    "\n",
    "**Objective:** Evaluate the performance of each model using metrics like accuracy, precision, recall, and F1 score, and select the best model based on these metrics.\n",
    "\n",
    "**Activities:**\n",
    "\n",
    "  1. **Gather predictions** from all previously implemented models (Logistic Regression, Decision Trees, Random Forests, SVM, KNN) on the test dataset.\n",
    "  2. **Calculate and compare** the accuracy of each model.\n",
    "  3. **Compute precision, recall, and F1 scores** for each model to evaluate their performance beyond mere accuracy.\n",
    "  4. **Calculate the ROC-AUC score** for each model to assess their overall ability to discriminate between positive and negative classes.\n",
    "  5. **Compare all models** based on the calculated test metrics and select the best performing model(s) for the churn prediction problem.\n",
    "  6. **Perform an error analysis** on the selected model to identify patterns in the misclassifications.\n",
    "  7. Based on the evaluation and comparison, **recommend the best model** for predicting customer churn and justify your recommendation.\n",
    "\n",
    "**Estimated Completion Time:** 90 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h1MLNndFZj0a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3sTyhBKvdu8E"
   },
   "source": [
    "**Hints:**\n",
    "\n",
    "* Compile predictions from all models and use `accuracy_score`, `precision_score`, `recall_score`, `f1_score`, and `roc_auc_score` for evaluation.\n",
    "* Create a summary table or visualization to compare model performances.\n",
    "* Conduct error analysis by reviewing the confusion matrix and misclassified examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZ99mVkvdu5h"
   },
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPgv031jxd1pYpj/2LkAy2D",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "techex",
   "language": "python",
   "name": "techex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
